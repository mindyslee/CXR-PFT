# Initialize metrics
mae_values = []
pearson_values = []
precision_values = []
recall_values = []
specificity_values = []
npv_values = []
accuracy_values = []

unique_patients = np.unique(patients)
selected_images_per_patient = {}

# Considering training and evaluation only on COPD patients
# copd_patients_indices = np.where(patients == 'COPD')[0]

for patient_id in unique_patients:
    patient_image_indices = np.where(patients == patient_id)[0]
    selected_image_index = np.random.choice(patient_image_indices)
    selected_images_per_patient[patient_id] = selected_image_index

selected_image_indices = np.array(list(selected_images_per_patient.values()))

#1202
train_indices, test_indices = train_test_split(
    selected_image_indices, 
    test_size=0.2, 
    random_state=42
)

# Split data into training and testing sets
x_train = image_data_rgb_batch[train_indices]
# 1202.21:11 Normalize input data to [0, 1] range
image_data_rgb_batch = image_data_rgb_batch / 255.0

x_trainB = sexagebmi_input[train_indices]
y_train = image_label[train_indices]

x_test = image_data_rgb_batch[test_indices]
x_testB = sexagebmi_input[test_indices]
y_test = image_label[test_indices]
#1202
#1202.21:11 Check for overlap between train and test sets
train_patient_ids = set(patients[train_indices])
test_patient_ids = set(patients[test_indices])

print("Overlap between train and test sets:", train_patient_ids & test_patient_ids)
if train_patient_ids & test_patient_ids:
    print("WARNING: There is overlap between train and test sets.")
##

# Histogram of true values
plt.hist(y_test, bins=20, alpha=0.7, color='blue', label='True Values')
plt.axvline(0, color='red', linestyle='--', label='Threshold (0)')
plt.xlabel("True Values")
plt.ylabel("Frequency")
plt.title("Distribution of True Values")
plt.legend()
plt.show()

inputA = Input(shape=(256, 256, 3,))
x = DenseNet121(weights='../densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5', include_top=False, input_tensor=inputA)
for layer in x.layers[:-1]:
    layer.trainable = False
x.layers[-1].trainable = True
# 12.02.21:11 Unfreeze the last 10 layers of DenseNet for better feature learning
for layer in x.layers[-10:]:
    layer.trainable = True

x = Flatten()(x.output)
x = Dense(256, activation="relu")(x)
x = BatchNormalization()(x)
x = Dropout(0.5)(x)
x = Dense(1, activation='linear')(x)
x = Model(inputs=inputA, outputs=x)

inputB = Input(shape=(3,))
y = Dense(9, activation="relu")(inputB)
y = Model(inputs=inputB, outputs=y)

combined = concatenate([x.output, y.output])
z = Dense(64, activation="relu")(combined)  # Increased to 64 units
z = Dropout(0.5)(z)
z = Dense(32, activation="relu")(z)  # Added another dense layer with 32 units
z = Dense(1, activation="linear")(z)

model = Model(inputs=[x.input, y.input], outputs=z)

# Define a custom weighted loss function
def weighted_mae(y_true, y_pred):
    weights = tf.where(y_true > 0, 2.0, 1.0)  # Weight errors for true values > 0 more heavily
    return tf.reduce_mean(weights * tf.abs(y_true - y_pred))

# 12.22.21:11 Add a learning rate scheduler
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=0.001,
    decay_steps=10000,
    decay_rate=0.9
)

# Compile model with custom loss
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),
              loss=weighted_mae,
              metrics=[MeanAbsoluteError(), MeanSquaredError()])


if "nv" in image_label and np.max(image_label) > 5:
    print("Skipping due to 'nv' category with values greater than 5.")
else:
    x_train, x_trainB = image_data_rgb_batch[selected_image_indices], sexagebmi_input[selected_image_indices]
    y_train = image_label[selected_image_indices]

    # Early stopping
    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    # Train the model 1202
    history = model.fit(x=[x_train, x_trainB], y=y_train,
                        batch_size=batch_size,
                        epochs=100,
                        verbose=1,
                        validation_data=([x_test, x_testB], y_test),
                        callbacks=[early_stopping])

    # Make predictions on the test set
    predictions = model.predict([x_test, x_testB])
    # residual plot new addition
    residuals = y_test - predictions.flatten()
    plt.scatter(y_test, residuals)
    plt.axhline(0, color='red', linestyle='--')
    plt.xlabel("True Values")
    plt.ylabel("Residuals (True - Predicted)")
    plt.title("Residuals vs True Values")
    plt.show()
    # 12.02.21:11 Plot residuals broken down by true value ranges
    plt.scatter(y_test, residuals, alpha=0.5, label='Residuals')
    plt.axhline(0, color='red', linestyle='--')
    plt.xlabel("True Values")
    plt.ylabel("Residuals")
    plt.title("Residuals Across True Values")
    plt.legend()
    plt.show()



    # 12.02.21:11 Dynamically calculate the 10th percentile threshold
    z_score_value = np.percentile(y_test, 10)

    true_value_binary_is_lowest = [1 if value <= z_score_value else 0 for value in y_test]

    # Step 2: Identify predicted labels in the lowest 10th percentile
    predict_value_binary_is_lowest = [1 if value <= z_score_value else 0 for value in predictions.flatten()]

    # Compare the true labels and predicted labels for the lowest 10%
    correct_predictions = sum(true_value == predict_value for true_value, predict_value in zip(true_value_binary_is_lowest, predict_value_binary_is_lowest))
    accuracy_lowest_10th_percentile = correct_predictions / len(true_value_binary_is_lowest)

    print(f"Accuracy for the lowest 10th percentile: {accuracy_lowest_10th_percentile:.2%}")

    # Calculate other metrics on the test set
    mae = mean_absolute_error(y_test, predictions)
    pearson_corr, _ = pearsonr(y_test, predictions.flatten())

    print(f"Mean Absolute Error: {mae:.2f}, Pearson Correlation: {pearson_corr:.2f}")

    confusion_mat = confusion_matrix(true_value_binary_is_lowest, predict_value_binary_is_lowest)
    tn, fp, fn, tp = confusion_mat.ravel()

    # Confusion matrix heatmap new addition
    confusion_mat = confusion_matrix(true_value_binary_is_lowest, predict_value_binary_is_lowest)
    plt.figure(figsize=(6, 4))
    sns.heatmap(confusion_mat, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Lowest 10%', 'Lowest 10%'], yticklabels=['Not Lowest 10%', 'Lowest 10%'])
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()


    precision = precision_score(true_value_binary_is_lowest, predict_value_binary_is_lowest)
    recall = recall_score(true_value_binary_is_lowest, predict_value_binary_is_lowest)
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    npv = tn / (tn + fn) if (tn + fn) > 0 else 0
    accuracy = accuracy_score(true_value_binary_is_lowest, predict_value_binary_is_lowest)

    print(f"Precision: {precision:.2f}, Recall: {recall:.2f}, Specificity: {specificity:.2f}, NPV: {npv:.2f}, Accuracy: {accuracy:.2f}")
    
    # Separate metrics for true values > 0 and <= 0
    positive_indices = np.where(y_test > 0)[0]
    negative_indices = np.where(y_test <= 0)[0]

    mae_positive = mean_absolute_error(y_test[positive_indices], predictions.flatten()[positive_indices])
    mae_negative = mean_absolute_error(y_test[negative_indices], predictions.flatten()[negative_indices])

    print(f"MAE for True Values > 0: {mae_positive:.2f}")
    print(f"MAE for True Values <= 0: {mae_negative:.2f}")

    # Scatter plot for true > 0
    plt.scatter(y_test[positive_indices], predictions.flatten()[positive_indices], color='green', alpha=0.6, label='True > 0')
    plt.scatter(y_test[negative_indices], predictions.flatten()[negative_indices], color='blue', alpha=0.6, label='True <= 0')
    plt.plot([-5, 5], [-5, 5], color='red', linestyle='--', label='Perfect Prediction Line')
    plt.xlabel("True Values")
    plt.ylabel("Predicted Values")
    plt.title("Predicted vs True Values (Separate for > 0 and <= 0)")
    plt.legend()
    plt.show()

    # Append metrics to tracking lists for later analysis
    mae_values.append(mae)
    pearson_values.append(pearson_corr)
    precision_values.append(precision)
    recall_values.append(recall)
    specificity_values.append(specificity)
    npv_values.append(npv)
    accuracy_values.append(accuracy)

    plt.scatter(y_test, predictions)
    plt.xlabel("True values")
    plt.ylabel("Predicted values")
    plt.title("True vs Predicted Values (Test Data)")
    plt.show()
    #1202

    
    del model
    del x_train
    del x_test #1202
    del y_train
    del y_test #1202
    gc.collect()
    K.clear_session()
    #print(1+"b")

# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

average_mae = round(sum(mae_values) / len(mae_values), 2)
average_pearson = round(sum(pearson_values) / len(pearson_values), 2)

print(f"\nAverage Mean Absolute Error: {average_mae:.2f}, Average Pearson Correlation: {average_pearson:.2f}")

average_precision = round(sum(precision_values) / len(precision_values), 2)
average_recall = round(sum(recall_values) / len(recall_values), 2)
average_specificity = round(sum(specificity_values) / len(specificity_values), 2)
average_npv = round(sum(npv_values) / len(npv_values), 2)
average_accuracy = round(sum(accuracy_values) / len(accuracy_values), 2)

print(f"\nAverage Precision: {average_precision:.2f}, Average Recall: {average_recall:.2f}, Average Specificity: {average_specificity:.2f}, Average NPV: {average_npv:.2f}, Average Accuracy: {average_accuracy:.2f}")
