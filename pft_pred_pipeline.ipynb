{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, confusion_matrix,\n",
    "                             mean_absolute_error, mean_squared_error)\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization, Input, concatenate\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import MeanAbsoluteError, MeanSquaredError\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# ——————————————————————————————\n",
    "# Data loading and preprocessing\n",
    "# ——————————————————————————————\n",
    "with open('../image_names_data_fvc_fev_fevfvc_dlco_z_sexage_bmi_labels.pkl', 'rb') as f:\n",
    "    (image_names, image_data, label_dict) = pickle.load(f)\n",
    "\n",
    "print(label_dict.keys())\n",
    "\n",
    "image_names = np.array(image_names)\n",
    "image_data = np.array(image_data)\n",
    "image_label = np.array(label_dict['fev1fvc_Z_label'])\n",
    "sex_label = np.array(label_dict['sex_label'])\n",
    "age_label = np.array(label_dict['age_label'])\n",
    "bmi_label = np.array(label_dict['bmi_label'])\n",
    "\n",
    "patients = np.array([x.split(\"_\")[0] for x in image_names])\n",
    "print(\"Number of Images:\", len(patients))\n",
    "print(\"Number of Unique Patients:\", len(set(patients)))\n",
    "\n",
    "# ——————————————————————————————\n",
    "# Data Cleaning\n",
    "# ——————————————————————————————\n",
    "temp_names, temp_data, temp_label = [], [], []\n",
    "temp_patients, temp_sex, temp_age, temp_bmi = [], [], [], []\n",
    "\n",
    "for index in range(len(image_names)):\n",
    "    if str(image_label[index]) in ['nv', 'None', 'nan']:\n",
    "        continue\n",
    "    if float(image_label[index]) > 5:\n",
    "        continue\n",
    "    if str(bmi_label[index]) in ['nan', 'None']:\n",
    "        continue\n",
    "    temp_names.append(image_names[index])\n",
    "    temp_data.append(image_data[index])\n",
    "    temp_label.append(float(image_label[index]))\n",
    "    temp_patients.append(patients[index])\n",
    "    temp_sex.append(sex_label[index])\n",
    "    temp_age.append(age_label[index])\n",
    "    temp_bmi.append(bmi_label[index])\n",
    "    \n",
    "image_names = np.array(temp_names)\n",
    "image_data = np.array(temp_data)\n",
    "image_label = np.array(temp_label)\n",
    "patients = np.array(temp_patients)\n",
    "sex_label = temp_sex\n",
    "age_label = temp_age\n",
    "bmi_label = temp_bmi\n",
    "\n",
    "# Flatten image data (if needed for other processing)\n",
    "image_data_flattened = image_data.reshape(image_data.shape[0], -1)\n",
    "print(\"Shape of image_data after reshaping:\", image_data_flattened.shape)\n",
    "print(\"Sizes:\", len(image_names), len(image_data), len(image_label))\n",
    "print('Min:', np.min(image_label))\n",
    "print('Mean:', np.mean(image_label))\n",
    "print('Median:', np.percentile(image_label, 50))\n",
    "print('75 percentile:', np.percentile(image_label, 90))\n",
    "print('Max:', np.max(image_label))\n",
    "\n",
    "total_people = len(image_label)\n",
    "temp = [1 if x < -1.67 else 0 for x in image_label]\n",
    "print(\"Less than -1.67\", sum(temp)/total_people)\n",
    "\n",
    "# histogram plot of image labels\n",
    "plt.hist(image_label, bins=10)\n",
    "plt.title(\"Histogram of Image Labels\")\n",
    "plt.xlabel(\"Image Label\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# ——————————————————————————————\n",
    "# Data Reshaping and Preparation\n",
    "# ——————————————————————————————\n",
    "image_size = (256, 256)\n",
    "batch_size = 32\n",
    "\n",
    "# Expand dims and convert grayscale to RGB\n",
    "image_data_channel_last = np.expand_dims(image_data, axis=3)\n",
    "image_data_rgb_batch = np.repeat(image_data_channel_last, 3, axis=-1)\n",
    "\n",
    "# Free memory\n",
    "del image_data\n",
    "gc.collect()\n",
    "\n",
    "# Prepare additional input (sex, age, bmi)\n",
    "sexagebmi_input = []\n",
    "for index in range(len(sex_label)):\n",
    "    s = 1 if sex_label[index] == \"Male\" else 0\n",
    "    sexagebmi_input.append([s, int(age_label[index]), int(float(bmi_label[index]))])\n",
    "sexagebmi_input = np.array(sexagebmi_input)\n",
    "print(\"Shape of sexagebmi_input:\", sexagebmi_input.shape)\n",
    "\n",
    "# ——————————————————————————————\n",
    "# Train-Test Split\n",
    "# ——————————————————————————————\n",
    "# Normalize the image data to [0, 1]\n",
    "image_data_rgb_batch = image_data_rgb_batch / 255.0\n",
    "\n",
    "# Select one image per unique patient\n",
    "unique_patients = np.unique(patients)\n",
    "selected_images_per_patient = {}\n",
    "for patient_id in unique_patients:\n",
    "    patient_image_indices = np.where(patients == patient_id)[0]\n",
    "    selected_image_index = np.random.choice(patient_image_indices)\n",
    "    selected_images_per_patient[patient_id] = selected_image_index\n",
    "selected_image_indices = np.array(list(selected_images_per_patient.values()))\n",
    "\n",
    "# Split selected indices into training and testing sets\n",
    "train_indices, test_indices = train_test_split(\n",
    "    selected_image_indices, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create training and testing sets\n",
    "x_train = image_data_rgb_batch[train_indices]\n",
    "x_trainB = sexagebmi_input[train_indices]\n",
    "y_train = image_label[train_indices]\n",
    "\n",
    "x_test = image_data_rgb_batch[test_indices]\n",
    "x_testB = sexagebmi_input[test_indices]\n",
    "y_test = image_label[test_indices]\n",
    "\n",
    "# Check for patient overlap between train and test sets\n",
    "train_patient_ids = set(patients[train_indices])\n",
    "test_patient_ids = set(patients[test_indices])\n",
    "print(\"Overlap between train and test sets:\", train_patient_ids & test_patient_ids)\n",
    "if train_patient_ids & test_patient_ids:\n",
    "    print(\"WARNING: There is overlap between train and test sets.\")\n",
    "\n",
    "# Plot distribution of true test values\n",
    "plt.hist(y_test, bins=20, alpha=0.7, color='blue', label='True Values')\n",
    "plt.axvline(0, color='red', linestyle='--', label='Threshold (0)')\n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of True Values\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ——————————————————————————————\n",
    "# Model Definition\n",
    "# ——————————————————————————————\n",
    "# Branch A: Image Input with DenseNet121\n",
    "inputA = Input(shape=(256, 256, 3))\n",
    "base_model = DenseNet121(weights='../densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5', \n",
    "                         include_top=False, input_tensor=inputA)\n",
    "for layer in base_model.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "base_model.layers[-1].trainable = True\n",
    "for layer in base_model.layers[-10:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(256, activation=\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1, activation='linear')(x)\n",
    "branchA = Model(inputs=inputA, outputs=x)\n",
    "\n",
    "# Branch B: Sex, Age, BMI Input\n",
    "inputB = Input(shape=(3,))\n",
    "y = Dense(9, activation=\"relu\")(inputB)\n",
    "branchB = Model(inputs=inputB, outputs=y)\n",
    "\n",
    "# Combined model\n",
    "combined = concatenate([branchA.output, branchB.output])\n",
    "z = Dense(64, activation=\"relu\")(combined)\n",
    "z = Dropout(0.5)(z)\n",
    "z = Dense(32, activation=\"relu\")(z)\n",
    "z = Dense(1, activation=\"linear\")(z)\n",
    "\n",
    "model = Model(inputs=[branchA.input, branchB.input], outputs=z)\n",
    "\n",
    "# Custom weighted MAE loss function\n",
    "def weighted_mae(y_true, y_pred):\n",
    "    weights = tf.where(y_true > 0, 2.0, 1.0)\n",
    "    return tf.reduce_mean(weights * tf.abs(y_true - y_pred))\n",
    "\n",
    "# Learning rate scheduler\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.001,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9\n",
    ")\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "              loss=weighted_mae,\n",
    "              metrics=[MeanAbsoluteError(), MeanSquaredError()])\n",
    "\n",
    "# ——————————————————————————————\n",
    "# Model Training and Evaluation\n",
    "# ——————————————————————————————\n",
    "# Initialize metrics storage\n",
    "mae_values = []\n",
    "pearson_values = []\n",
    "precision_values = []\n",
    "recall_values = []\n",
    "specificity_values = []\n",
    "npv_values = []\n",
    "accuracy_values = []\n",
    "\n",
    "if \"nv\" in image_label and np.max(image_label) > 5:\n",
    "    print(\"Skipping due to 'nv' category with values greater than 5.\")\n",
    "else:\n",
    "    # Reassign training data using selected images per patient\n",
    "    x_train = image_data_rgb_batch[selected_image_indices]\n",
    "    x_trainB = sexagebmi_input[selected_image_indices]\n",
    "    y_train = image_label[selected_image_indices]\n",
    "\n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(x=[x_train, x_trainB], y=y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=100,\n",
    "                        verbose=1,\n",
    "                        validation_data=([x_test, x_testB], y_test),\n",
    "                        callbacks=[early_stopping])\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    predictions = model.predict([x_test, x_testB])\n",
    "    residuals = y_test - predictions.flatten()\n",
    "    \n",
    "    # Plot residuals (single consolidated plot)\n",
    "    plt.scatter(y_test, residuals, alpha=0.5, label='Residuals')\n",
    "    plt.axhline(0, color='red', linestyle='--')\n",
    "    plt.xlabel(\"True Values\")\n",
    "    plt.ylabel(\"Residuals (True - Predicted)\")\n",
    "    plt.title(\"Residuals vs True Values\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate binary labels for the lowest 10th percentile using dynamic threshold\n",
    "    z_score_value = np.percentile(y_test, 10)\n",
    "    true_value_binary_is_lowest = [1 if value <= z_score_value else 0 for value in y_test]\n",
    "    predict_value_binary_is_lowest = [1 if value <= z_score_value else 0 for value in predictions.flatten()]\n",
    "\n",
    "    correct_predictions = sum(1 for true_val, pred_val in zip(true_value_binary_is_lowest, predict_value_binary_is_lowest)\n",
    "                              if true_val == pred_val)\n",
    "    accuracy_lowest_10th_percentile = correct_predictions / len(true_value_binary_is_lowest)\n",
    "    print(f\"Accuracy for the lowest 10th percentile: {accuracy_lowest_10th_percentile:.2%}\")\n",
    "\n",
    "    # Calculate and print regression metrics\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    pearson_corr, _ = pearsonr(y_test, predictions.flatten())\n",
    "    print(f\"Mean Absolute Error: {mae:.2f}, Pearson Correlation: {pearson_corr:.2f}\")\n",
    "\n",
    "    confusion_mat = confusion_matrix(true_value_binary_is_lowest, predict_value_binary_is_lowest)\n",
    "    tn, fp, fn, tp = confusion_mat.ravel()\n",
    "    \n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(confusion_mat, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Not Lowest 10%', 'Lowest 10%'],\n",
    "                yticklabels=['Not Lowest 10%', 'Lowest 10%'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    precision = precision_score(true_value_binary_is_lowest, predict_value_binary_is_lowest)\n",
    "    recall = recall_score(true_value_binary_is_lowest, predict_value_binary_is_lowest)\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "    accuracy_metric = accuracy_score(true_value_binary_is_lowest, predict_value_binary_is_lowest)\n",
    "    \n",
    "    print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, Specificity: {specificity:.2f}, NPV: {npv:.2f}, Accuracy: {accuracy_metric:.2f}\")\n",
    "    \n",
    "    # Evaluate performance separately for true values > 0 and <= 0\n",
    "    positive_indices = np.where(y_test > 0)[0]\n",
    "    negative_indices = np.where(y_test <= 0)[0]\n",
    "    \n",
    "    mae_positive = mean_absolute_error(y_test[positive_indices], predictions.flatten()[positive_indices])\n",
    "    mae_negative = mean_absolute_error(y_test[negative_indices], predictions.flatten()[negative_indices])\n",
    "    print(f\"MAE for True Values > 0: {mae_positive:.2f}\")\n",
    "    print(f\"MAE for True Values <= 0: {mae_negative:.2f}\")\n",
    "    \n",
    "    # Scatter plot: Predicted vs True values for different true value ranges\n",
    "    plt.scatter(y_test[positive_indices], predictions.flatten()[positive_indices], color='green', alpha=0.6, label='True > 0')\n",
    "    plt.scatter(y_test[negative_indices], predictions.flatten()[negative_indices], color='blue', alpha=0.6, label='True <= 0')\n",
    "    plt.plot([-5, 5], [-5, 5], color='red', linestyle='--', label='Perfect Prediction Line')\n",
    "    plt.xlabel(\"True Values\")\n",
    "    plt.ylabel(\"Predicted Values\")\n",
    "    plt.title(\"Predicted vs True Values (Separated by True Value Sign)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Overall scatter plot for test data\n",
    "    plt.scatter(y_test, predictions.flatten())\n",
    "    plt.xlabel(\"True values\")\n",
    "    plt.ylabel(\"Predicted values\")\n",
    "    plt.title(\"True vs Predicted Values (Test Data)\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Append metrics for later analysis\n",
    "    mae_values.append(mae)\n",
    "    pearson_values.append(pearson_corr)\n",
    "    precision_values.append(precision)\n",
    "    recall_values.append(recall)\n",
    "    specificity_values.append(specificity)\n",
    "    npv_values.append(npv)\n",
    "    accuracy_values.append(accuracy_metric)\n",
    "    \n",
    "    del model, x_train, x_test, y_train, y_test\n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "\n",
    "# ——————————————————————————————\n",
    "# Plot Training and Validation Loss\n",
    "# ——————————————————————————————\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# ——————————————————————————————\n",
    "# Average Metrics Reporting\n",
    "# ——————————————————————————————\n",
    "average_mae = round(sum(mae_values) / len(mae_values), 2)\n",
    "average_pearson = round(sum(pearson_values) / len(pearson_values), 2)\n",
    "print(f\"\\nAverage Mean Absolute Error: {average_mae:.2f}, Average Pearson Correlation: {average_pearson:.2f}\")\n",
    "\n",
    "average_precision = round(sum(precision_values) / len(precision_values), 2)\n",
    "average_recall = round(sum(recall_values) / len(recall_values), 2)\n",
    "average_specificity = round(sum(specificity_values) / len(specificity_values), 2)\n",
    "average_npv = round(sum(npv_values) / len(npv_values), 2)\n",
    "average_accuracy = round(sum(accuracy_values) / len(accuracy_values), 2)\n",
    "print(f\"\\nAverage Precision: {average_precision:.2f}, Average Recall: {average_recall:.2f}, \"\n",
    "      f\"Average Specificity: {average_specificity:.2f}, Average NPV: {average_npv:.2f}, \"\n",
    "      f\"Average Accuracy: {average_accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
